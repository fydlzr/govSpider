# -*- coding: utf-8 -*-
import sys
reload(sys)
sys.setdefaultencoding('utf-8')
# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import os
from scrapy.exceptions import DropItem
import pymongo
from scrapy.conf import settings
from pybloom import BloomFilter
import hashlib

keywords = set(['incentive',
	'facility',
	'real estate',
	'property',
	'REIT',
	'REITs',
	'housing',
	'Chinese investors',
	'foreign investment',
	'insentif',
	'fasilitas',
	'properti',
	'perumahan',
	'pelaburan langsung asing',
	'PLA',	
])

class GovspiderPipeline(object):
    def __init__(self):
	#self.urls_seen = set()
	if os.path.exists(settings['MONGODB_DB']+'.urls'):
                self.bloomFilter = BloomFilter.fromfile(open(settings['MONGODB_DB']+'.urls','r'))
	else:
		self.bloomFilter = BloomFilter(100000000,0.001)
        connection = pymongo.MongoClient(
            settings['MONGODB_SERVER'],
            settings['MONGODB_PORT']
        )
        db = connection[settings['MONGODB_DB']]
        self.collection = db[settings['MONGODB_COLLECTION']]

    def process_item(self, item, spider):
	if self.url_seen(item['url'])
		raise DropItem("Duplicate item found: %s" % md5)
	else:
		self.bloomFilter.add(md5)
	if self.hasKeyword(item['lang'],item['content']):
		self.collection.insert(dict(item))
	else:
		print 'no keywords: '+ item['url']
		raise DropItem("no Keywords:%s" %item['url'])
        return item

    def open_spider(self, spider):
        spider.myPipeline = self
        #for url in self.collection.find({},{"url":1,"_id":0}):
        #        self.urls_seen.add(url["url"])

    def close_spider(self, spider):
	self.bloomFilter.tofile(open(settings['MONGODB_DB']+'.urls','wb'))

    def hasKeyword(self, lang, text):
	if lang!='en':return True
        for key in keywords:
                if key in text:
                        return True
        return False
